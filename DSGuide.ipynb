{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data Science Guide\n### Karl Evans 2024/2025","metadata":{}},{"cell_type":"markdown","source":"## 1. Getting Started - Systems\n## 1.1 Gitlab\n","metadata":{}},{"cell_type":"code","source":"#Git\ngit clone \\\ngit pull \\\ngit add \\\ngit commit -m 'something' \\\ngit push \\\ngit branch \\\ngti status \\\ngit checkout ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1.2 Bash/Shell","metadata":{}},{"cell_type":"code","source":"htop \\\npwd \\\nls \\\ncd \\\nmkdir \\\ntouch \\\n.. \\\nrm \\\nfm -rf \\\n\\","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1.3 Containerisation\n### - Environments\n","metadata":{}},{"cell_type":"markdown","source":"## - Virtual Environment \nA virtual environment creates a folder that contains a copy (or symlink) to a specific interpreter. When you install packages into a virtual environment it will end up in this new folder, and thus isolated from other packages used by other workspaces.","metadata":{}},{"cell_type":"code","source":"python -m venv example-env\n# Windows\nexample-env\\Scripts\\activate\n# Unix/MacOS\nsource example-env/bin/activate\ndeactivate","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### - Conda\nconda environments define what software is available for a project  \nEnvironments in conda are self-contained, isolated spaces where you can install specific versions of software packages, including dependencies, libraries, and Python versions. This isolation helps avoid conflicts between package versions and ensures that your projects have the exact libraries and tools they need. ","metadata":{}},{"cell_type":"code","source":"conda env list \\\nconda create --name <> python =<> \\\nconda activate \\\nconda deactivate \\\nconda env remove --name <> \\","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### - Jupyter Kernels\nKernels are programming language specific processes that run independently and interact with the Jupyter Applications and their user interfaces. ipykernel is the reference Jupyter kernel built on top of IPython, providing a powerful environment for interactive computing in Python.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"python -m pip install package\npython -m pip freeze \npython -m pip list\npython -m pip show","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Data Analysis\n## 2.1 SQL","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2.2 Data Structures","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2.3 EDA","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Modelling","metadata":{}},{"cell_type":"markdown","source":"## 3.1 Feature Engineering\n - Linear models learn sums and differences naturally, but can't learn anything more complex.\n - Ratios seem to be difficult for most models to learn. Ratio combinations often lead to some easy performance gains.\n - Linear models and neural nets generally do better with normalized features. Neural nets especially need features scaled to values not too far from 0. Tree-based models (like random forests and XGBoost) can sometimes benefit from normalization, but usually much less so.\n - Tree models can learn to approximate almost any combination of features, but when a combination is especially important they can still benefit from having it explicitly created, especially when data is limited.\n - Counts are especially helpful for tree models, since these models don't have a natural way of aggregating information across many features at once.","metadata":{}},{"cell_type":"markdown","source":"### - Categorical","metadata":{}},{"cell_type":"code","source":"OneHotEncoding\nLabelEncoding\nFrequencyEncoding\nMeanEncoding\nTargetEncoding","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### - Numeric","metadata":{}},{"cell_type":"code","source":"Smoothing\nClipping\nZ-score scaling\nLinear scaling\nlog scaling\n# Binning\n# - Good for 1) non-linear 2) clustered\n\n.gt(0).sum(axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ignore_features = ['reliability','intrnt_email_addr','clnt_intrnl_id']\n\ncat_col = [col for col, type in df.dtypes.items() if type in [\"category\", 'object']and col not in ignore_features]\nnum_col = [col for col, type in df.dtypes.items() if type in ['float', 'int64']and col not in ignore_features]\nfeatures = num_col + cat_col\n\ny=df['reliability']\nX=df.drop(['reliability','intrnt_email_addr','clnt_intrnl_id'], axis=1)  df['reliability'] = np.where(df['reliability'] == \"Unreliable\", 1, 0)\ny= df['reliability'].values","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"PCA\nproduct or sum for high same sign features\nratio or subtract high opposite sign features","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Clustering\nkmeans = KMeans(n_clusters=6)\nX[\"Cluster\"] = kmeans.fit_predict(X)\nX[\"Cluster\"] = X[\"Cluster\"].astype(\"category\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Missing Data","metadata":{}},{"cell_type":"code","source":"Mean/Median Imputation\n\"-999\" for tree based models\nMost Frequent Imputation\n\"Miss\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.2 Pipelines","metadata":{}},{"cell_type":"code","source":"# EG catboost\nfrom catboost import CatBoostClassifier\n\ndef build_pipeline(num_features: List[str], cat_features: List[str]) -> Pipeline:\n    \"\"\"Full pipeline\n\n    This function constructs the whole pipeline for training\n\n    Params:\n        config (Dict): Config content from yaml\n        num_features (List[str]): List of numeric features\n        cat_features (List[str]): List of categorial feature\n\n    Returns:\n        Pipeline that contians pre-process, sampling (If specified) and model\n\n    Note:\n        * Config assumes we're in the `pipeline` level already\n    \"\"\"\n    numeric_transformer = make_pipeline(SimpleImputer(strategy='mean'),\n                                       StandardScaler())\n\n    categorical_transformer = make_pipeline(SimpleImputer(strategy='most_frequent'),\n        #SimpleImputer(strategy='constant', fill_value='missing'),\n        OneHotEncoder(handle_unknown='ignore', min_frequency=0.05))\n\n    preprocessor = make_column_transformer((numeric_transformer, num_features),\n        (categorical_transformer, cat_features),\n        remainder=\"passthrough\")\n    \n    pipe = Pipeline(steps=[\n        ('preprocessor', preprocessor),\n        ('model', CatBoostClassifier())\n    ])\n    return pipe\n\npipeline = build_pipeline(num_features=num_col, cat_features=cat_col)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.3 Cross Validation\n","metadata":{}},{"cell_type":"markdown","source":"### Feature Importance","metadata":{}},{"cell_type":"markdown","source":"Mutual Information","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import mutual_info_regression\n\ndef make_mi_scores(X, y, discrete_features):\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\nmi_scores = make_mi_scores(X, y, discrete_features)\nmi_scores[::3]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.4 Class Imbalance\n\n#### - Oversampling\n\n#### - Undersampling","metadata":{}},{"cell_type":"markdown","source":"## 3.5 Dimensionality Reduction\n#### - PCA\n#### - t-SNE\n#### - LDA\n#### - ICA\n#### - UMAP","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.6 Ensembling\n#### - Bagging\n#### - Boosting\n#### - Stacking/Meta ","metadata":{}},{"cell_type":"markdown","source":"### Association\n#### - ","metadata":{}},{"cell_type":"markdown","source":"## 3.7 Hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import optuna\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\n\nX_train, X_test,y_train, y_test=train_test_split(X,y, test_size=0.2, random_state=21)\n\ndef objective(trial):\n    # Suggest hyperparameters\n    params = {\n        'model__iterations':trial.suggest_int(\"iterations\", , 500, step=1),\n        'model__depth': trial.suggest_int(\"depth\", 4, 10, step=1),,\n        'model__learning_rate':trial.suggest_int(\"learning_rate\", , 500, step=1),,\n        'model__cat_features':'',\n        'model__loss_function':'Logloss',\n        'model__verbose': 'True'     \n    }\n\niterations=2,\n                           depth=2,\n                           learning_rate=1,\n                           loss_function='Logloss',\n                           verbose=True)\n    \n    model = pipeline.set_params(**params)\n    scores = cross_val_score(model, X_train, y_train, cv=10, scoring='f1', verbose=3, error_score='raise')\n    \n    return scores.mean()\n#### - GridSearchCV\n#### - HyperOpt\n#### - Optuna\n  # Create a study object\nstudy = optuna.create_study(direction=\"maximize\")\n\n# Optimize the objective function\nstudy.optimize(objective, n_trials=10)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.1 Supervised Learning","metadata":{}},{"cell_type":"code","source":"\n### Linear Regression\n#### - Ridge\n#### - Lasso\n\n### Logistic Regression\n\n### Support Vector Machines\n\n### Randon Forests\n\n### K Nearest Neighbours\n\n### Naive Bayes\n\n### Gradient Boosting\n#### - XGBoost\n\n### Neural Networks\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Outlier Detection\n#### - Isolation Forest\n#### - One Class Classification\n","metadata":{}},{"cell_type":"code","source":"from sklearn.datasets import load_iris\nimport sklearn\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\niris = load_iris()\nX_train, X_test, y_train, y_test = train_test_split(iris.data, \n                                                    iris.target, \n                                                    test_size=0.4, \n                                                    random_state=1)\nsvm_class_1 = SVC()\nsvm_class_1.fit(X_train,y_train==1)\nplot_classifier(X_train, y_train==1,svm_class_1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.2 Unsupervised Learning","metadata":{}},{"cell_type":"code","source":"### Clustering\n#### - k-Means\n#### - Hierarchical\n#### - DBSCANS\n#### - Gaussian Mixture Models","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.4 Semi-Supervised Learning\n","metadata":{}},{"cell_type":"markdown","source":"## 3.5 Deep Learning","metadata":{}},{"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Define RNN layer\n        self.rnn = nn.RNN(\n            input_size=1,\n            hidden_size=32,\n            num_layers=2,\n            batch_first=True,\n        )\n        self.fc = nn.Linear(32, 1)\n\n    def forward(self, x):\n        # Initialize first hidden state with zeros\n        h0 = torch.zeros(2, x.size(0), 32)\n        # Pass x and h0 through recurrent layer\n        out, _ = self.rnn(x, h0)  \n        # Pass recurrent layer's last output through linear layer\n        out = self.fc(out[:, -1, :])\n        return out\n\t\t\n\t\t\nclass Net(nn.Module):\n    def __init__(self, input_size):\n        super().__init__()\n        # Define lstm layer\n        self.lstm = nn.LSTM(\n            input_size=1,\n            hidden_size=32,\n            num_layers=2,\n            batch_first=True,\n        )\n        self.fc = nn.Linear(32, 1)\n\n    def forward(self, x):\n        h0 = torch.zeros(2, x.size(0), 32)\n        # Initialize long-term memory\n        c0 =  torch.zeros(2, x.size(0), 32)\n        # Pass all inputs to lstm layer\n        out, _ = self.lstm(x,(h0,c0))\n        out = self.fc(out[:, -1, :])\n        return out\n\t\t\nclass Net(nn.Module):\n    def __init__(self, input_size):\n        super().__init__()\n        # Define RNN layer\n        self.gru = nn.GRU(\n            input_size=1,\n            hidden_size=32,\n            num_layers=2,\n            batch_first=True,\n        )\n        self.fc = nn.Linear(32, 1)\n\n    def forward(self, x):\n        h0 = torch.zeros(2, x.size(0), 32)\n        out, _ = self.gru(x, h0)  \n        out = self.fc(out[:, -1, :])\n        return out","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Reinforcement Learning\n","metadata":{}},{"cell_type":"markdown","source":"## 6. Metrics\n","metadata":{}},{"cell_type":"markdown","source":"## 7. Other\n### Causality\n#### - Propensity\n\n","metadata":{}},{"cell_type":"markdown","source":"## 8. Production\n#### 1. AP455.md\n#### 2. aep_manifest\n#### 3. entrypoint.sh","metadata":{}},{"cell_type":"markdown","source":"### Model Lifecycling\n#### - MLflow","metadata":{}}]}
